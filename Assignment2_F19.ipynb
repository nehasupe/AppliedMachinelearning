{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2-F19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehasupe/AppliedMachinelearning/blob/master/Assignment2_F19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R--G1S4LFVR9",
        "colab_type": "text"
      },
      "source": [
        "CSCI-P556: Applied Machine Learning\n",
        "Fall 2018\n",
        "Assignment #2\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooye3d-VGooZ",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1 (50 points)\n",
        "\n",
        "In this problem you will be working with Bank Marketing Dataset. We encourage your to look at the description and make yourself comfortable with it: https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
        "\n",
        "Two pairs of training and validation sets have been drawn from this dataset for you to work with, namely: \n",
        " -  `train_1.csv` and `valid_1.csv`\n",
        " -  `train_2.csv`and `valid_2.csv`\n",
        " \n",
        "Finally, we have also provided a `test.csv` which is common for both settings.\n",
        "\n",
        "### Task 0: (3 points)\n",
        "\n",
        "What is the difference between the two sets of train/validation sets? Write your answer here:\n",
        "\n",
        "### Ans: <!-- Your answers goes here -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFKZT4mhJlSK",
        "colab_type": "text"
      },
      "source": [
        "### Task 1 (Data Preprocessing) (12 points)\n",
        "\n",
        "Before you can train a machine learning model using this dataset, you will have to run a number of preprocessing steps to transform the data into a form which will be acceptable to the model. In this task, you are supposed to carry out the follow steps:\n",
        "\n",
        "1.  Find all features that have continuous numeric values and normalize them in the same way as you did in Assignment 1. List those features here.\n",
        "2. Find all categorical features and process them as was discussed in class. List those features and explain what you did.\n",
        "3. Are there any features which have missing values? (Hint: Missing values aren't always nicely represented and can be misleading sometimes). Find those features and explain how you will handle them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLoZ67_hPo9I",
        "colab_type": "text"
      },
      "source": [
        "Write your text-based answers here.\n",
        "\n",
        "\n",
        "1.   Answer for bullet one\n",
        "2.   Answer for bullet two\n",
        "3.   Answer for bullet three\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kxn7ZjCJpJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                # PROBLEM 1, TASK 1 CODE GOES HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EaChdKLJjio",
        "colab_type": "text"
      },
      "source": [
        "\\### Task 2 (Model $\\mathcal{A}$) (10 points)\n",
        "\n",
        "Train a logistic regression model with $L_2$ regularization on `train_1.csv` after applying the necessary preprocessing steps of Task 1. Use the validation set `valid_1.csv` to choose the best regularization coefficient $\\lambda$. Plot a curve with regularization coefficient in the X axis and the validation accuracy on the Y axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQGJ10L-KhZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PROBLEM 1, TASK 2 CODE GOES HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0UEYv92KnvQ",
        "colab_type": "text"
      },
      "source": [
        "### Task 3 (Model $\\mathcal{B}$) (5 points)\n",
        "Train a logistic regression model with $L_2$ regularization on `train_2.csv` after applying the necessary preprocessing steps of Task 1. Use the validation set `valid_2.csv` to choose the best regularization coefficient $\\lambda$. Plot a curve with regularization coefficient in the X axis and the validation accuracy on the Y axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izeiBcCTLcM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PROBLEM 1, TASK 3 CODE GOES HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KzsVhHPLd3P",
        "colab_type": "text"
      },
      "source": [
        "### Task 4 (Model $\\mathcal{C}$) (10 points)\n",
        "\n",
        "As you have seen in class, due to the specific (and smart) choice of the loss function, a logistic regression classifier happens to predict the posterioir probability $p(y|\\mathbf{x})$ for any given sample. It then classifies the samples with $p(y|\\mathbf{x}) > 0.5$ as belonging to class $1$ and the rest as belonging to class $0$. Instead, for a fixed threshold of $0.5$, let us consider a dynamic threshold of $\\theta$ which can be chosen after a model has been trained.\n",
        "\n",
        "$$\n",
        "\\ y=\n",
        "\\begin{cases}\n",
        "1 \\text{ if } p(y|\\mathbf{x}) \\geq \\theta \\\\\n",
        "\\\\\n",
        "0 \\text{ if } p(y|\\mathbf{x}) < \\theta \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "for $0 \\leq \\theta \\leq 1$.\n",
        "\n",
        "Train a logistic regression model with $L_2$ regularization on `train_2.csv` after applying the necessary preprocessing steps of Task 1. Use the validation set `valid_2.csv` to choose the best combination of regularization coefficient $\\lambda$ and the threshold $\\theta$. The best coefficients has to be selected by you after trying out at least five, non-consecutive values. Visualize all these runs.\n",
        "\n",
        "**Hint**: Look at the documentation of Logistic Regression in sci-kit learn to come up with a plan to implement this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJRVTjjrSTRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PROBLEM 1, TASK 4 CODE GOES HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTa2tMCPQmCV",
        "colab_type": "text"
      },
      "source": [
        "### Task 5 (Report and Conclusion) (10 points)\n",
        "\n",
        "1.  Give a detailed report on the performance (that is, accuracy) of Models $\\mathcal{A}$, $\\mathcal{B}$ and $\\mathcal{C}$ on the respective training sets, the validation sets and finally the test. Explain any intuition you might have on what might be happening in each of the cases. Please avoid writing unnecessary details which are not pertinent.\n",
        "2.  Do you think accuracy, the metric which you have used to find the best hyperparameter in all the above tasks, a good metric in this scenario? Explain your choice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b62zDpk-O8v",
        "colab_type": "text"
      },
      "source": [
        "## Problem 2 (50 points)\n",
        "\n",
        "Consider a regression problem in which the input variable $x$ has only one feature and the target variable $y$ is generated from the input variable $x$ by a polynomial $f(x)$. The function $f$ is such that $$f(x) = \\sum_{i=0}^d w_ix^i$$ Please note that a given polynomial function can be completely described by the vector of coefficients $\\mathbf{w}$.\n",
        "\n",
        "### Task 1 (5 points)\n",
        "\n",
        "Write a function in python which can take the degree of polynomial function as input. The code to generate vector coefficeints $\\mathbf{w}$ using this input is already provided. Plot the resulting function for $x \\in (-low, high)$. The function should be plot "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivzCo28P_SFW",
        "colab_type": "code",
        "outputId": "e8db8c72-79f9-4aa5-aa97-8779bc3bd024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import legendre\n",
        "import random\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_coefficients(degree):\n",
        "    \"\"\"\n",
        "    The coefficients returned are in the order w_0, w_1, ... , w_{degree}\n",
        "    \"\"\"\n",
        "    return legendre(degree).coefficients[::-1]\n",
        "\n",
        "def visualize_function(degree, low, high):\n",
        "\ty = 0\n",
        "\tx = np.random.uniform(low, high, 1)\n",
        "\tw = get_coefficients(degree)\n",
        "  for i in range(degree+1):\n",
        "    y = y + w[i] * pow(x, i)\n",
        "\t#print(x, y)\n",
        "\t#plt.plot(x, y,'ro')\n",
        "\t#plt.xlabel('x - axis')\n",
        "\t#plt.ylabel('f(x) - axis')\n",
        "\t#plt.title('Visualize function')\n",
        "\t#plt.show()\n",
        "\treturn (x, y)\n",
        "## TEST CASES (The following lines should plot a function)\n",
        "# visualize_funtion(3, -1, 1)\n",
        "# visualize_funtion(4, -1, 1)\n",
        "# visualize_funtion(6, -1, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-779e7ca35c6d>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    for i in range(degree+1):\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Mm1YGNBH5u",
        "colab_type": "text"
      },
      "source": [
        "### Task 2 (5 points)\n",
        "\n",
        "Write a function in python that randomly generates a dataset (training data + test data). The function should take the degree of target polynomial as input along with the number of points that should be generated for both the training and the test sets. The polynomial will be represented by the vector of coefficients. The code to calculate these coefficients has been provided. The function should also add random normal noise to the target -- i.e. the noise should be drawn from a normal distribution with mean $\\mu=0$ and standard deviation = $\\sigma^2$. Thus: \n",
        "$$y=\\sum_{i=0}^d w_ix^i + \\mathcal{N}(0, \\sigma^2)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ1zxLPvCyDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_dataset(degree, n_train, n_test, sigma):\n",
        "    \"\"\"\n",
        "    - n_train - Number of training samples\n",
        "    - n_test - Number of test samples\n",
        "    - X_train is an array of size (n_train $\\times$ 1)\n",
        "    - y_train is an array of size (n_train $\\times$ 1)\n",
        "    - X_test is an array of size (n_test $\\times$ 1)\n",
        "    - y_test is an array of size (n_test $\\times$ 1)\n",
        "    \"\"\"\n",
        "    w = get_coefficients(degree)\n",
        "    X_train = [0]*n_train\n",
        "    y_train = [0]*n_train\n",
        "    X_test = [0]*n_test\n",
        "    y_test = [0]*n_test\n",
        "    for i in range(n_train):\n",
        "    \t(x, y) = visualize_function(degree, -1, 1)\n",
        "    \ty = y + np.random.normal(0, pow(sigma, 2),size=None)\n",
        "    \tX_train[i] = x\n",
        "    \ty_train[i] = y\n",
        "    for i in range(n_test):\n",
        "    \t(x, y) = visualize_function(degree, -1, 1)\n",
        "    \t#y = y + np.random.normal(0, pow(sigma, 2),size=None)\n",
        "    \tX_test[i] = x\n",
        "    \ty_test[i] = y\n",
        "    #plt.plot(X_train, y_train,'ro')\n",
        "    #plt.show()\n",
        "    #plt.plot(X_test, y_test,'ro')\n",
        "    #plt.show()\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et9gTUzy_Sn9",
        "colab_type": "text"
      },
      "source": [
        "### Task 3 (20 points)\n",
        "\n",
        "1.   Use the function written in Task 2 to generate a dataset of degree $10$, with $20$ samples each in the training and test sets. The standard deviation of the noise should be $0.2$.\n",
        "2.   Plot the generated points of the train set over the function. Make use the function you wrote in Task 1 to do so. The resultant plot should not only contain the function in blue color but also the generated points in the train set in red color.\n",
        "3.   Fit two linear regressions models: $\\mathcal{A}$ and $\\mathcal{B}$ on the given training data. Model $\\mathcal{A}$ should be quadratic and Model $\\mathcal{B}$ should be of degree $10$. How would you create such models?\n",
        "4. Calculate the training error and testing error on both these models and report them nicely in a table.\n",
        "5. Explain the results:\n",
        " - Are the results as expected or surprising? Explain your stance.\n",
        " - Give a detailed explanation of why this happens.\n",
        " \n",
        " **Hint**: You are free (in fact encouraged) to design new experiments under the same paradigm to understand what is happening. You can buttress your explanation with any plots which you might generate from such new experiments.\n",
        "\n",
        " \n",
        "Note: _There is no preference over the size of the answer but please note that you shouldn't write anything that is not pertinent. While being terse, your answer should clearly convey the intuition. Also, use logical number code/text cells---i.e. every subquestion in a given task which can be considered a logical unit should be present in one cell. Do not make one cell unnecessarily long by putting multiple answers in it or split a given answer over multiple cells. Use your best judgement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5XghFfHwxje",
        "colab_type": "code",
        "outputId": "1bdf026c-b17c-438a-e1f9-bb052c17a55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "(X_train, y_train, X_test, y_test) = generate_dataset(10, 20, 20, 0.2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_train = poly.fit_transform(X_train)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_poly_pred = model.predict(X_train)\n",
        "print(model.coef)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.00000000e+00 -6.19367793e-01  3.83616463e-01]\n",
            " [ 1.00000000e+00  3.82131007e-01  1.46024107e-01]\n",
            " [ 1.00000000e+00  7.33714682e-01  5.38337234e-01]\n",
            " [ 1.00000000e+00  1.44513376e-01  2.08841160e-02]\n",
            " [ 1.00000000e+00 -2.30993568e-01  5.33580285e-02]\n",
            " [ 1.00000000e+00 -5.87092616e-01  3.44677739e-01]\n",
            " [ 1.00000000e+00  3.69618120e-01  1.36617554e-01]\n",
            " [ 1.00000000e+00  8.17967068e-01  6.69070125e-01]\n",
            " [ 1.00000000e+00  6.11196831e-03  3.73561566e-05]\n",
            " [ 1.00000000e+00 -4.07821870e-01  1.66318678e-01]\n",
            " [ 1.00000000e+00  1.57418899e-01  2.47807098e-02]\n",
            " [ 1.00000000e+00 -9.07648417e-01  8.23825649e-01]\n",
            " [ 1.00000000e+00  1.90357563e-01  3.62360017e-02]\n",
            " [ 1.00000000e+00  3.21801029e-01  1.03555902e-01]\n",
            " [ 1.00000000e+00 -9.21258974e-01  8.48718097e-01]\n",
            " [ 1.00000000e+00 -4.43042588e-01  1.96286735e-01]\n",
            " [ 1.00000000e+00  4.69504042e-01  2.20434045e-01]\n",
            " [ 1.00000000e+00 -9.03427249e-01  8.16180795e-01]\n",
            " [ 1.00000000e+00  9.35065153e-01  8.74346840e-01]\n",
            " [ 1.00000000e+00  1.04888337e-01  1.10015632e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APUqEUWUGmXo",
        "colab_type": "text"
      },
      "source": [
        "### Task 4 (20 points)\n",
        "\n",
        "1.  Use the function written in Task 2 to generate a dataset of degree 45, with 20 samples each in the training and test sets. There should be no noise in the samples.\n",
        "2.  Repeat steps 2 to 4 from Task 3 for this dataset.\n",
        "3.  This target doesn't have noise like the previous one. Does this make the result surprising? Explain your stance. Also give a detailed explanation of why this happens.\n",
        "\n",
        "**Hint**: You are free (in fact encouraged) to design new experiements under the same paradigm to understand what is happening. You can buttress your explanation with any plots which you might generate from such new experiments."
      ]
    }
  ]
}