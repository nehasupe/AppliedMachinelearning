{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_P556_F19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehasupe/AppliedMachinelearning/blob/master/A3_P556_F19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg9fRSjFtsRu",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #3\n",
        "## P556: Applied Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjsZpx5C9eBH",
        "colab_type": "text"
      },
      "source": [
        "More often than not, we will use a deep learning library (Tensorflow, Pytorch, or the wrapper known as Keras) to implement our models. However, the abstraction afforded by those libraries can make it hard to troubleshoot issues if we don't understand what is going on under the hood. In this assignment you will implement a fully-connected and a convolutional neural network from scratch. To simplify the implementation, we are asking you to implement static architectures, but you are free to support variable number of layers/neurons/activations/optimizers/etc. We recommend that you make use of private methods so you can easily troubleshoot small parts of your model as you develop them, instead of trying to figure out which parts are not working correctly after implementing everything. Also, keep in mind that there is code from your fully-connected neural network that can be re-used on the CNN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NzW9M-btzqO",
        "colab_type": "text"
      },
      "source": [
        "Problem #1.1 (40 points): Implement a fully-connected neural network from scratch. The neural network will have the following architecture:\n",
        "\n",
        "- Input layer\n",
        "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
        "- Dropout with a value of 0.2\n",
        "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
        "- Dropout with a value of 0.2\n",
        "- Output layer, using softmax as the activation function\n",
        "\n",
        "The model will use categorical crossentropy as its loss function. \n",
        "We will optimize the gradient descent using RMSProp, with a learning rate of 0.001 and a rho value of 0.9.\n",
        "We will evaluate the model using accuracy.\n",
        "\n",
        "Why this architecture? We are trying to reproduce from scratch the following [example from the Keras documentation](https://keras.io/examples/mnist_mlp/). This means that you can compare your results by running the Keras code provided above to see if you are on the right track."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rPUmRqBtpS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# For this assignment I have watched the following 2 courses on Coursera,\n",
        "# https://www.coursera.org/learn/neural-networks-deep-learning/\n",
        "# https://www.coursera.org/learn/deep-neural-network\n",
        "# The theory, notations and vectorized implementations used in this assignment are from the course\n",
        "# Referrence links mainly mentioned in the assignment are links to the videos from these two courses\n",
        "\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "  \"\"\"\n",
        "  forward propagation:\n",
        "  input layer is a row in x_train- Randomly initialize weights between -1 and 1\n",
        "  2 dense hidden layer- with 512 neurons use a relu function and drop value of 0.2\n",
        "  output layer- softmax, output y_train\n",
        "\n",
        "  backward propagation:\n",
        "  loss function- categorical entropy\n",
        "  gradient descent- optimize using RMSProp, learning value 0.001, rho 0.9\n",
        "\n",
        "  model evaluation:\n",
        "  accuracy\n",
        "  \"\"\"\n",
        "\n",
        "  '''\n",
        "  Hyperparameters:\n",
        "  learning rate - 0.001\n",
        "  #iterations- \n",
        "  #hidden layers\n",
        "  #hidden units\n",
        "  choice of activation function\n",
        "  dropout - 0.2\n",
        "  '''\n",
        "\n",
        "  def __init__(self, epochs, batch_size, drop_out):\n",
        "    self.epochs = epochs\n",
        "    self.learning_rate = 0.001\n",
        "    self.rho = 0.9\n",
        "    self.batch_size = batch_size\n",
        "    self.keep_prob = 1 - drop_out\n",
        "    self.Sdw1, self.Sdb1, self.Sdw2, self.Sdb2, self.Sdw3, self.Sdb3 = 0, 0, 0, 0, 0, 0\n",
        "    \n",
        "# Initializes Parameters of the model and returns the initialized parameters\n",
        "  def initialize_parameters(self, layers):\n",
        "    # Initializing the weights W1, W2, W3 and biases b1, b2, b3 for the hidden layer 1, hidden layer 2 and the Output layer\n",
        "    # layer dimensions are 784 (layer 0 or input layer), 512 (hidden layer 1), 512 (hidden layer 2), 10(output layer or number of classes)\n",
        "    \n",
        "    # Previously initialized the Weights this way, resulted in overflow in softmax activation\n",
        "    # so then multiplied it by 0.01 to make the values closer to zero as suggested by Andrew Ng in one of his videos in the above listed courses\n",
        "    # W1 = np.random.uniform(-1, 1, layers[1] * layers[0]) \n",
        "    # W1 = W1.reshape(layers[1], layers[0])\n",
        "    # W2 = np.random.uniform(-1, 1, layers[2] * layers[1]) \n",
        "    # W2 = W2.reshape(layers[2], layers[1])\n",
        "    # W3 = np.random.uniform(-1, 1, layers[3] * layers[2])\n",
        "    # W3 = W3.reshape(layers[3], layers[2])\n",
        "\n",
        "    # Wl is of shape - (layer[l], layer[l-1])\n",
        "    # W1 - (512, 784), W2 - (512, 512), W3 - (10, 512)\n",
        "    W1 = np.random.uniform(-1, 1, layers[1] * layers[0]) * 0.01\n",
        "    W1 = W1.reshape(layers[1], layers[0])\n",
        "    W2 = np.random.uniform(-1, 1, layers[2] * layers[1]) * 0.01\n",
        "    W2 = W2.reshape(layers[2], layers[1])\n",
        "    W3 = np.random.uniform(-1, 1, layers[3] * layers[2]) * 0.01\n",
        "    W3 = W3.reshape(layers[3], layers[2])\n",
        "    \n",
        "    # bl is of shape - (layer[l], 1)\n",
        "    # b1 - (512, 1), b2 - (512, 1), b3 - (10, 1)\n",
        "    b1 = np.zeros((layers[1], 1))\n",
        "    b2 = np.zeros((layers[2], 1))\n",
        "    b3 = np.zeros((layers[3], 1))\n",
        "\n",
        "    return W1, W2, W3, b1, b2, b3\n",
        "\n",
        "\n",
        "# For forward_dropout and backward_dropout\n",
        "# Referred Coursera video: 'Drop out regularization' in Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization (Week 1)\n",
        "# https://www.coursera.org/learn/deep-neural-network/lecture/eM33A/dropout-regularization\n",
        "# Dropout regularization using technique called inverted dropout\n",
        "  def forward_dropout(self, A):\n",
        "    D = np.random.rand(A.shape[0], A.shape[1]) < self.keep_prob\n",
        "    A = np.multiply(A, D)\n",
        "    A /= self.keep_prob\n",
        "    return A, D\n",
        "\n",
        "  def backward_dropout(self, W, dZ, D):\n",
        "    dA = np.dot(W.T, dZ)\n",
        "    dA = np.multiply(dA, D)\n",
        "    dA /= self.keep_prob\n",
        "    return dA\n",
        "\n",
        "# For implementation of RMSProp\n",
        "# Ref. https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop\n",
        "# Returns updated values of weight and bias\n",
        "  def RMSProp(self, W, b, dW, db, Sdw, Sdb):\n",
        "    # With epsilon value 10 ^ -8 as said by Andrew Ng in his video, accuracy was low\n",
        "    # Try different values of epsilon: 10 ^ -3, 10 ^ -4\n",
        "    epsilon = math.pow(10, -4)\n",
        "    Sdw = self.rho * Sdw + (1 - self.rho) * np.multiply(dW, dW)\n",
        "    Sdb = self.rho * Sdb + (1 - self.rho) * np.multiply(db, db)\n",
        "    W = W - self.learning_rate * np.divide(dW, np.power(Sdw, 0.5) + epsilon)\n",
        "    b = b - self.learning_rate * np.divide(db, np.power(Sdb, 0.5) + epsilon)\n",
        "    return W, b, Sdw, Sdb\n",
        "\n",
        "# For softmax activation, derivative and cost function\n",
        "# Ref. https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier\n",
        "# https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy\n",
        "# Returns the cost using categorical cross entropy function for softmax\n",
        "  def Cost(self, Y_dash, Y):\n",
        "      m = Y.shape[1]\n",
        "      cost = (-1/m) * np.sum(np.multiply(Y, np.log(Y_dash)))\n",
        "      return cost\n",
        "\n",
        "# For Forward Propagation function and Back Propagation function \n",
        "# Referred the vectorized implementation from the following resources\n",
        "# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Wh8NI/gradient-descent-for-neural-networks\n",
        "# returns predicted Y, weights, biases and activations\n",
        "\n",
        "  def forward_propagation(self, X, W1, W2, W3, b1, b2, b3):\n",
        "    ### Hidden Layer 1, 512 neurons, Relu activation function, dropout = 0.2 \n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.maximum(Z1, 0)  #RELU activation\n",
        "    A1, D1 = self.forward_dropout(A1) #Dropout\n",
        "\n",
        "    ### Hidden Layer 2, 512 neurons, Relu activation function, dropout = 0.2\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = np.maximum(Z2, 0)  # RELU activation\n",
        "    A2, D2 = self.forward_dropout(A2) #Dropout\n",
        "\n",
        "    ### Output layer, 10 classes, Softmax activation function\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    # Softmax Acivation\n",
        "    # A3 = np.exp(Z3) / np.sum(np.exp(Z3), axis=0) gives overflow error\n",
        "    # Ref. for error: https://stackoverflow.com/questions/54880369/implementation-of-softmax-function-returns-nan-for-high-inputs\n",
        "    f = np.exp(Z3 - np.max(Z3))    \n",
        "    A3 = f / np.sum(f, axis=0)\n",
        "\n",
        "    Y_dash = A3\n",
        "    activations = (A1, A2, A3)\n",
        "    weights = (W1, W2, W3)\n",
        "    biases = (b1, b2, b3)\n",
        "    D = (D1, D2)\n",
        "\n",
        "    return Y_dash, activations, weights, biases, D\n",
        "\n",
        "  def backward_propagation(self, X, Y, activations, weights, biases, D):    \n",
        "    m = X.shape[1]\n",
        "    (A1, A2, A3) = activations\n",
        "    (W1, W2, W3) = weights\n",
        "    (b1, b2, b3) = biases\n",
        "    (D1, D2) = D\n",
        "    \n",
        "    ### Output layer, softmax derivative\n",
        "    dZ3 = A3 - Y # Softmax derivative\n",
        "    dW3 = (1 / m) * np.dot(dZ3, A2.T)\n",
        "    db3 = (1 / m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "    dA2 = self.backward_dropout(W3, dZ3, D2) # Dropout\n",
        "\n",
        "    ### Hidden layer, Relu derivative\n",
        "    # RELU derivative g'(z) = 0 if z < 0, g'(z) = 1 if z > 0\n",
        "    dZ2 = np.multiply(dA2, A2 > 0) # RELU derivative\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    \n",
        "    dA1 = self.backward_dropout(W2, dZ2, D1) # Dropout\n",
        "\n",
        "    # Hidden layer, Relu derivative\n",
        "    dZ1 = np.multiply(dA1, A1 > 0) # RELU derivative\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    return dW3, dW2, dW1, db3, db2, db1\n",
        "\n",
        "# Fit function fits the passed train set, performing mini batch gradient descent\n",
        "# The Keras implementation makes use of 20 epochs and batches of size 128\n",
        "# Mini batch gradient descent: https://www.youtube.com/watch?v=4qJaSmvhxi8\n",
        "# Returns weights and biases of the trained model\n",
        "  def fit(self, X, Y, layers):\n",
        "    W1, W2, W3, b1, b2, b3 = self.initialize_parameters(layers)\n",
        "    for j in range(0, self.epochs):\n",
        "      print(\"epoch :\", j+1)\n",
        "      # Creating batches of size 128, 375 batches for the 48000 samples\n",
        "      for i in range(0, X.shape[0], self.batch_size):\n",
        "        # forward propagation\n",
        "        Y_dash, activations, weights, biases, D = self.forward_propagation(X[i:i+self.batch_size].T, W1, W2, W3, b1, b2, b3)\n",
        "        # Compute Cost\n",
        "        # cost = self.Cost(Y_dash, Y[i:i+self.batch_size].T)\n",
        "        # print(\"cost:\",cost)\n",
        "        # Backward propagation\n",
        "        dW3, dW2, dW1, db3, db2, db1 = self.backward_propagation(X[i:i+self.batch_size].T, Y[i:i+self.batch_size].T, activations, weights, biases, D)\n",
        "        # Using RMSProp to optimize gradient descent and update weights and biases\n",
        "        W3, b3, self.Sdw3, self.Sdb3 = self.RMSProp(W3, b3, dW3, db3, self.Sdw3, self.Sdb3)\n",
        "        W2, b2, self.Sdw2, self.Sdb2 = self.RMSProp(W2, b2, dW2, db2, self.Sdw2, self.Sdb2)\n",
        "        W1, b1, self.Sdw1, self.Sdb1 = self.RMSProp(W1, b1, dW1, db1, self.Sdw1, self.Sdb1)\n",
        "      # print cost after each epoch\n",
        "      cost = self.Cost(Y_dash, Y[i:i+self.batch_size].T)\n",
        "      print(\"cost:\",cost)    \n",
        "\n",
        "    return W1, W2, W3, b1, b2, b3\n",
        "\n",
        "# Predict function with activations and no dropout, returns the vector of predicted values\n",
        "  def predict(self, X, W1, W2, W3, b1, b2, b3):\n",
        "    ### Hidden Layer 1, 512 neurons, Relu activation function\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.maximum(Z1, 0)  #RELU activation\n",
        "\n",
        "    ### Hidden Layer 2, 512 neurons, Relu activation function\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = np.maximum(Z2, 0)  # RELU activation\n",
        "\n",
        "    ### Output layer, 10 classes, Softmax activation function\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    # Softmax Acivation\n",
        "    # A3 = np.exp(Z3) / np.sum(np.exp(Z3), axis=0) gives overflow error\n",
        "    # Ref. for error: https://stackoverflow.com/questions/54880369/implementation-of-softmax-function-returns-nan-for-high-inputs\n",
        "    f = np.exp(Z3 - np.max(Z3))    \n",
        "    A3 = f / np.sum(f, axis=0)\n",
        "\n",
        "    return A3\n",
        "\n",
        "  # Evaluation function for the Neural Network, evaluates using the Accuracy metric\n",
        "  def evaluate(self, X, Y, W1, W2, W3, b1, b2, b3):\n",
        "    Y_dash = self.predict(X.T, W1, W2, W3, b1, b2, b3)\n",
        "    Y_dash = np.argmax(Y_dash, axis = 0)\n",
        "    Y = np.argmax(Y, axis = 1)\n",
        "    # Accuracy = Number of correct predictions/ Total number of samples \n",
        "    # Ref. for getting a count of same elements in 2 numpy arrays: \n",
        "    # https://stackoverflow.com/questions/25490641/check-how-many-elements-are-equal-in-two-numpy-arrays-python/25490691\n",
        "    accuracy = np.sum(Y_dash == Y)/Y.shape[0]\n",
        "    return accuracy\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH3bgJyPuE2O",
        "colab_type": "text"
      },
      "source": [
        "Problem #1.2 (10 points): Train your fully-connected neural network on the Fashion-MNIST dataset using 5-fold cross validation. Report accuracy on the folds, as well as on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsN4sUoUugl8",
        "colab_type": "code",
        "outputId": "f544c41e-d4dd-454b-9bef-072d3a922582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# To simplify the usage of our dataset, we will be importing it from the Keras \n",
        "# library. Keras can be installed using pip: python -m pip install keras\n",
        "\n",
        "# Original source for the dataset:\n",
        "# https://github.com/zalandoresearch/fashion-mnist\n",
        "\n",
        "# Reference to the Fashion-MNIST's Keras function: \n",
        "# https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "import keras.utils\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape[0], 'Rows: train samples')\n",
        "print(x_train.shape[1], 'Columns: train samples')\n",
        "print(x_test.shape[0], 'Rows: test samples')\n",
        "print(x_test.shape[1], 'Columns: test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Dimensions of layer: Number of inputs and neurons in each layer of neural Network\n",
        "layers = [ 784, 512, 512, 10]\n",
        "\n",
        "# K- fold validation from the official documentation\n",
        "# Referred the example here for working of K-fold- \n",
        "# https://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#sphx-glr-auto-examples-exercises-plot-cv-diabetes-py\n",
        "kf = KFold(n_splits=5)\n",
        "fold_train_accuracy = []\n",
        "fold_test_accuracy = []\n",
        "test_dataset_accuracy = []\n",
        "\n",
        "# For each fold, fitting the model with train set, evaluating it with the test set and also on the given test dataset\n",
        "for i , (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
        "  print(\"Fold:\", i+1)\n",
        "  nn = NeuralNetwork(epochs = 20, batch_size = 128, drop_out = 0.2)\n",
        "  W1, W2, W3, b1, b2, b3 = nn.fit(x_train[train_index], y_train[train_index], layers)\n",
        "\n",
        "  train_accuracy = nn.evaluate(x_train[train_index], y_train[train_index], W1, W2, W3, b1, b2, b3)\n",
        "  print(\"For fold:\", i+1)\n",
        "  print(\"Train accuracy:\",train_accuracy)\n",
        "  fold_train_accuracy.append(train_accuracy)\n",
        "\n",
        "  test_accuracy = nn.evaluate(x_train[test_index], y_train[test_index], W1, W2, W3, b1, b2, b3)\n",
        "  print(\"Test accuracy:\",test_accuracy)\n",
        "  fold_test_accuracy.append(test_accuracy)\n",
        "\n",
        "  dataset_accuracy = nn.evaluate(x_test, y_test, W1, W2, W3, b1, b2, b3)\n",
        "  print(\"Accuracy on the Test set\", dataset_accuracy)\n",
        "  test_dataset_accuracy.append(dataset_accuracy)\n",
        "\n",
        "print(\"5-fold cross validation\")\n",
        "print(\"The overall (average) training accuracy\", sum(fold_train_accuracy)/len(fold_train_accuracy))\n",
        "print(\"The overall(average) testing accuracy\", sum(fold_test_accuracy)/len(fold_test_accuracy))\n",
        "print(\"The overall(average) accuracy on the test dataset\", sum(test_dataset_accuracy)/len(test_dataset_accuracy))\n",
        "\n",
        "print(\"fitting the entire train set to the model and testing on the given test dataset\")\n",
        "nn = NeuralNetwork(epochs = 20, batch_size = 128, drop_out = 0.2)\n",
        "\n",
        "W1, W2, W3, b1, b2, b3 = nn.fit(x_train, y_train, layers)\n",
        "accuracy = nn.evaluate(x_train, y_train, W1, W2, W3, b1, b2, b3)\n",
        "print(\"Train set accuracy\", accuracy)\n",
        "\n",
        "accuracy = nn.evaluate(x_test, y_test, W1, W2, W3, b1, b2, b3)\n",
        "print(\"Test set accuracy\", accuracy)\n",
        "\"\"\"\n",
        "Comparing results with the Keras implementation:\n",
        "The link included in the question is for keras implementation on the mnist dataset\n",
        "I executed the code for keras implementation to get the output on mnist dataset:\n",
        "Test accuracy: 0.9821\n",
        "\n",
        "Results of this implementation on Mnist dataset:\n",
        "The overall (average) training accuracy 0.9980958333333334\n",
        "The overall(average) testing accuracy 0.9784833333333334\n",
        "The overall(average) accuracy on the test dataset 0.9804999999999999\n",
        "Train set accuracy 0.9985833333333334\n",
        "Test set accuracy 0.9845\n",
        "\n",
        "I executed the Keras implementation on the fashion-mnist dataset and got a test accuracy of 0.8742\n",
        "\n",
        "Results of this implementation on Fashion mnist dataset:\n",
        "\n",
        "5-fold cross validation\n",
        "The overall (average) training accuracy 0.9220083333333333\n",
        "The overall(average) testing accuracy 0.8908833333333334\n",
        "The overall(average) accuracy on the test dataset 0.8828400000000001\n",
        "fitting the entire train set to the model and testing on the given test dataset\n",
        "Train set accuracy 0.9236\n",
        "Test set accuracy 0.8859\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "60000 Rows: train samples\n",
            "784 Columns: train samples\n",
            "10000 Rows: test samples\n",
            "784 Columns: test samples\n",
            "Fold: 1\n",
            "epoch : 1\n",
            "cost: 0.207351412885461\n",
            "epoch : 2\n",
            "cost: 0.19364492279156803\n",
            "epoch : 3\n",
            "cost: 0.1877477439888224\n",
            "epoch : 4\n",
            "cost: 0.17659390155531832\n",
            "epoch : 5\n",
            "cost: 0.12820119806586872\n",
            "epoch : 6\n",
            "cost: 0.1578474904465153\n",
            "epoch : 7\n",
            "cost: 0.1666165928596442\n",
            "epoch : 8\n",
            "cost: 0.1790876128865162\n",
            "epoch : 9\n",
            "cost: 0.12590151816571862\n",
            "epoch : 10\n",
            "cost: 0.15956686059747738\n",
            "epoch : 11\n",
            "cost: 0.14291726518899317\n",
            "epoch : 12\n",
            "cost: 0.12079236107607953\n",
            "epoch : 13\n",
            "cost: 0.08654107598329869\n",
            "epoch : 14\n",
            "cost: 0.08820797107630336\n",
            "epoch : 15\n",
            "cost: 0.06289329547388212\n",
            "epoch : 16\n",
            "cost: 0.036908168556412116\n",
            "epoch : 17\n",
            "cost: 0.1042599603229974\n",
            "epoch : 18\n",
            "cost: 0.04801526708002827\n",
            "epoch : 19\n",
            "cost: 0.07148436927748822\n",
            "epoch : 20\n",
            "cost: 0.09138009489514881\n",
            "For fold: 1\n",
            "Train accuracy: 0.9981458333333333\n",
            "Test accuracy: 0.98025\n",
            "Accuracy on the Test set 0.9789\n",
            "Fold: 2\n",
            "epoch : 1\n",
            "cost: 0.23791045636513308\n",
            "epoch : 2\n",
            "cost: 0.2677455078375732\n",
            "epoch : 3\n",
            "cost: 0.19722404581315314\n",
            "epoch : 4\n",
            "cost: 0.1807969707835243\n",
            "epoch : 5\n",
            "cost: 0.17059636344020257\n",
            "epoch : 6\n",
            "cost: 0.16093043758864606\n",
            "epoch : 7\n",
            "cost: 0.14888788686069762\n",
            "epoch : 8\n",
            "cost: 0.17254115497639835\n",
            "epoch : 9\n",
            "cost: 0.15306775347417312\n",
            "epoch : 10\n",
            "cost: 0.16007953876978825\n",
            "epoch : 11\n",
            "cost: 0.1755111195917454\n",
            "epoch : 12\n",
            "cost: 0.09313021781328047\n",
            "epoch : 13\n",
            "cost: 0.16647724477755838\n",
            "epoch : 14\n",
            "cost: 0.12665049971350503\n",
            "epoch : 15\n",
            "cost: 0.1666055020361741\n",
            "epoch : 16\n",
            "cost: 0.09357756799639541\n",
            "epoch : 17\n",
            "cost: 0.06470956701177624\n",
            "epoch : 18\n",
            "cost: 0.10351170407921101\n",
            "epoch : 19\n",
            "cost: 0.09089827032583195\n",
            "epoch : 20\n",
            "cost: 0.07804690403236943\n",
            "For fold: 2\n",
            "Train accuracy: 0.997875\n",
            "Test accuracy: 0.9786666666666667\n",
            "Accuracy on the Test set 0.9804\n",
            "Fold: 3\n",
            "epoch : 1\n",
            "cost: 0.29989782798525755\n",
            "epoch : 2\n",
            "cost: 0.2585872696739327\n",
            "epoch : 3\n",
            "cost: 0.17443633274641918\n",
            "epoch : 4\n",
            "cost: 0.2105976799752506\n",
            "epoch : 5\n",
            "cost: 0.2159807080640922\n",
            "epoch : 6\n",
            "cost: 0.2209365931303467\n",
            "epoch : 7\n",
            "cost: 0.19356976439906215\n",
            "epoch : 8\n",
            "cost: 0.14546908641559103\n",
            "epoch : 9\n",
            "cost: 0.09725180076239805\n",
            "epoch : 10\n",
            "cost: 0.17644440258888391\n",
            "epoch : 11\n",
            "cost: 0.1178470117687919\n",
            "epoch : 12\n",
            "cost: 0.18574781610972205\n",
            "epoch : 13\n",
            "cost: 0.16438891903966515\n",
            "epoch : 14\n",
            "cost: 0.09888634481327548\n",
            "epoch : 15\n",
            "cost: 0.10388544323378315\n",
            "epoch : 16\n",
            "cost: 0.07475835362972906\n",
            "epoch : 17\n",
            "cost: 0.09492261674705013\n",
            "epoch : 18\n",
            "cost: 0.08844599216928874\n",
            "epoch : 19\n",
            "cost: 0.07639207677494803\n",
            "epoch : 20\n",
            "cost: 0.061368984772840626\n",
            "For fold: 3\n",
            "Train accuracy: 0.997625\n",
            "Test accuracy: 0.9794166666666667\n",
            "Accuracy on the Test set 0.9808\n",
            "Fold: 4\n",
            "epoch : 1\n",
            "cost: 0.22250976656145507\n",
            "epoch : 2\n",
            "cost: 0.2298498987108152\n",
            "epoch : 3\n",
            "cost: 0.2687913427810328\n",
            "epoch : 4\n",
            "cost: 0.18061599540312473\n",
            "epoch : 5\n",
            "cost: 0.14841027084081065\n",
            "epoch : 6\n",
            "cost: 0.19109476075917092\n",
            "epoch : 7\n",
            "cost: 0.16666695689168648\n",
            "epoch : 8\n",
            "cost: 0.11967467046297099\n",
            "epoch : 9\n",
            "cost: 0.18150299553149563\n",
            "epoch : 10\n",
            "cost: 0.14342678553450822\n",
            "epoch : 11\n",
            "cost: 0.17975969504988504\n",
            "epoch : 12\n",
            "cost: 0.16872364380827665\n",
            "epoch : 13\n",
            "cost: 0.09514450546444787\n",
            "epoch : 14\n",
            "cost: 0.09644917571723854\n",
            "epoch : 15\n",
            "cost: 0.12137299460982319\n",
            "epoch : 16\n",
            "cost: 0.08568391748229888\n",
            "epoch : 17\n",
            "cost: 0.11096666004363724\n",
            "epoch : 18\n",
            "cost: 0.05091221136973599\n",
            "epoch : 19\n",
            "cost: 0.06468733397006567\n",
            "epoch : 20\n",
            "cost: 0.07047971149937488\n",
            "For fold: 4\n",
            "Train accuracy: 0.9984791666666667\n",
            "Test accuracy: 0.97475\n",
            "Accuracy on the Test set 0.9798\n",
            "Fold: 5\n",
            "epoch : 1\n",
            "cost: 0.24530988741895549\n",
            "epoch : 2\n",
            "cost: 0.1796315396060718\n",
            "epoch : 3\n",
            "cost: 0.11646796138821881\n",
            "epoch : 4\n",
            "cost: 0.10364312501278738\n",
            "epoch : 5\n",
            "cost: 0.058229847627933914\n",
            "epoch : 6\n",
            "cost: 0.043750045125699515\n",
            "epoch : 7\n",
            "cost: 0.06567467131529785\n",
            "epoch : 8\n",
            "cost: 0.051512573096282814\n",
            "epoch : 9\n",
            "cost: 0.018473913241843935\n",
            "epoch : 10\n",
            "cost: 0.01620153632397661\n",
            "epoch : 11\n",
            "cost: 0.013938622861109608\n",
            "epoch : 12\n",
            "cost: 0.020515696366316317\n",
            "epoch : 13\n",
            "cost: 0.024783719201518485\n",
            "epoch : 14\n",
            "cost: 0.03236093494569147\n",
            "epoch : 15\n",
            "cost: 0.02017876048011118\n",
            "epoch : 16\n",
            "cost: 0.040006648147002076\n",
            "epoch : 17\n",
            "cost: 0.04523885637934203\n",
            "epoch : 18\n",
            "cost: 0.006946922543767008\n",
            "epoch : 19\n",
            "cost: 0.0026501752002302164\n",
            "epoch : 20\n",
            "cost: 0.013621205114654313\n",
            "For fold: 5\n",
            "Train accuracy: 0.9983541666666667\n",
            "Test accuracy: 0.9793333333333333\n",
            "Accuracy on the Test set 0.9826\n",
            "5-fold cross validation\n",
            "The overall (average) training accuracy 0.9980958333333334\n",
            "The overall(average) testing accuracy 0.9784833333333334\n",
            "The overall(average) accuracy on the test dataset 0.9804999999999999\n",
            "fitting the entire train set to the model and testing on the given test dataset\n",
            "epoch : 1\n",
            "cost: 0.30254192706337363\n",
            "epoch : 2\n",
            "cost: 0.324648161370386\n",
            "epoch : 3\n",
            "cost: 0.2602442788661491\n",
            "epoch : 4\n",
            "cost: 0.2338315073403671\n",
            "epoch : 5\n",
            "cost: 0.24025530465539718\n",
            "epoch : 6\n",
            "cost: 0.21955897740269786\n",
            "epoch : 7\n",
            "cost: 0.22472154842117567\n",
            "epoch : 8\n",
            "cost: 0.2176155612728672\n",
            "epoch : 9\n",
            "cost: 0.18417455951162215\n",
            "epoch : 10\n",
            "cost: 0.12291159172199928\n",
            "epoch : 11\n",
            "cost: 0.21822846182165462\n",
            "epoch : 12\n",
            "cost: 0.14093890329643521\n",
            "epoch : 13\n",
            "cost: 0.1225324005781796\n",
            "epoch : 14\n",
            "cost: 0.15825473794790426\n",
            "epoch : 15\n",
            "cost: 0.0703542419326034\n",
            "epoch : 16\n",
            "cost: 0.06892718580908533\n",
            "epoch : 17\n",
            "cost: 0.13766178738947288\n",
            "epoch : 18\n",
            "cost: 0.10874401553324131\n",
            "epoch : 19\n",
            "cost: 0.042483786321324886\n",
            "epoch : 20\n",
            "cost: 0.060287716141560435\n",
            "Train set accuracy 0.9985833333333334\n",
            "Test set accuracy 0.9845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nResults of this implementation on Fashion mnist dataset\\n\\n5-fold cross validation\\nThe overall (average) training accuracy 0.9220083333333333\\nThe overall(average) testing accuracy 0.8908833333333334\\nThe overall(average) accuracy on the test dataset 0.8828400000000001\\nfitting the entire train set to the model and testing on the given test dataset\\nTrain set accuracy 0.9236\\nTest set accuracy 0.8859\\n\\nResults of this implementation on Mnist dataset\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}
