{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment4_F19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehasupe/AppliedMachinelearning/blob/master/assignment4_F19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOmqxSwoVei5",
        "colab_type": "text"
      },
      "source": [
        "CSCI P-556: Applied Machine Learning\n",
        "\n",
        "Fall 2019\n",
        "\n",
        "Assignment 4\n",
        "\n",
        "Due: 11:59PM, December 6, 2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZy3rcBEeUC8",
        "colab_type": "text"
      },
      "source": [
        "Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMLcX2kmeUL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "train = pd.read_csv('a4-train.csv', index_col=0)\n",
        "test = pd.read_csv('a4-test.csv', index_col=0)\n",
        "\n",
        "y_train = train['labels']\n",
        "x_train = train.drop(['labels'], axis=1)\n",
        " \n",
        "\n",
        "y_test = test['labels']\n",
        "x_test = test.drop(['labels'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT3Oh9KcdWYd",
        "colab_type": "text"
      },
      "source": [
        "Task 1: Perform exploratory data analysis (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HL6FR5FxHas",
        "colab_type": "text"
      },
      "source": [
        "Reference: https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15\n",
        "\n",
        "On performing exploratory data analysis following is the inferrence:\n",
        "Both the train set and test set are balanced. Since the labels in the train set are balanced, we can use accuracy as a metric to evaluate our models. All the 500 features are numerical features of int64 type.There are no missing values. The heatmap shows a few of the features have a high correlation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us3rhe68VPb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train set\n",
        "print(\"Exploratory Data Analysis:\")\n",
        "print(\"Train set:\")\n",
        "print(\"Train Shape:\", train.shape)\n",
        "print(\"Number of samples:\", train.shape[0])\n",
        "print(\"Number of features:\", train.shape[1])\n",
        "print(\"Train head, 5 first samples\")\n",
        "print(train.head())\n",
        "print(\"Train information:\")\n",
        "print(train.info())\n",
        "\n",
        "print(\"Train description:\")\n",
        "print(train.describe())\n",
        "\n",
        "print(\"Target variable:\")\n",
        "print(\" target labels\", train.labels.unique())\n",
        "print(\"Each label count:\")\n",
        "print(train.labels.value_counts())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APLU_Cqz-LZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Graphical analysis of data:\")\n",
        "# Reference: https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15\n",
        "sns.pairplot(x_train, hue = 'labels', size = 10)\n",
        "plt.show()\n",
        "print(\"Heatmap for missing values:\")\n",
        "sns.heatmap(train.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')\n",
        "plt.figure(figsize=(60,40))\n",
        "sns.heatmap(train.corr(),cmap='Blues',annot=False)\n",
        "\n",
        "print(\"Correlation Heatmap:\")\n",
        "k = 501 #number of variables for heatmap\n",
        "cols = train.corr().nlargest(k, 'labels')['labels'].index\n",
        "cm = train[cols].corr()\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(cm, annot=True, cmap = 'viridis')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcWUL4-JFVtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Test set\n",
        "\n",
        "print(\"Test set\")\n",
        "\n",
        "print(\"Test shape\", test.shape)\n",
        "print(\"Test head\")\n",
        "print(test.head())\n",
        "print(\"Test Information\")\n",
        "print(test.info())\n",
        "print(\"Test Description\")\n",
        "print(test.describe())\n",
        "print(\"Test target variable labels\")\n",
        "print(test.labels.unique())\n",
        "print(\"Test target variable label counts\")\n",
        "print(test.labels.value_counts())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_ilhT9HnKKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference: https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15\n",
        "print(\"Graphical analysis of data:\")\n",
        "print(\"Heatmap for missing values in test set\")\n",
        "sns.heatmap(test.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')\n",
        "plt.figure(figsize=(60,40))\n",
        "sns.heatmap(test.corr(),cmap='Blues',annot=False)\n",
        "\n",
        "print(\"Heatmap showing correlation amongst the features:\")\n",
        "k = 501 #number of variables for heatmap\n",
        "cols = test.corr().nlargest(k, 'labels')['labels'].index\n",
        "cm = test[cols].corr()\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(cm, annot=True, cmap = 'viridis')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6RNWshKMZe2",
        "colab_type": "text"
      },
      "source": [
        "We can observe that there are no missing values.\n",
        "The train as well as test set is balanced # what to do of negative correlation values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55slmaFsdvLu",
        "colab_type": "text"
      },
      "source": [
        "Task 2: Use scikit-learn's logistic regression to establish a baseline model. We are not expecting this model to perform well, we just want to know what is the highest accuracy that we can achieve without doing any feature engineering and/or parameter tuning. (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa1sxtLkphvd",
        "colab_type": "text"
      },
      "source": [
        "On performing Logistic Regression on the given data without any feature engineering gives accuracy of 59% on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_qi7_QMeH4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(x_train, y_train)\n",
        "y_predict = logreg.predict(x_test)\n",
        "\n",
        "train_score = logreg.score(x_train, y_train)\n",
        "test_score = logreg.score(x_test, y_test)\n",
        "print(\"Train set score:\", train_score)\n",
        "print(\"Test set score\", test_score)\n",
        "# Train set score: 0.745\n",
        "# Test set score 0.59"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xPtBfYieKt7",
        "colab_type": "text"
      },
      "source": [
        "Task 3: Feature engineering. On this task you'll write code to modify your features such that the model can achieve a higher accuracy. You are free to modify the data as you want, but a rationale has to be provided for each modification. Additionally, if you are using anything that has not been convered in class, write a brief description of how the function works. Note that copy/pasting a function's docstring is not what we are asking and we will take off points if this is what you do. (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmgvdl7GAX1C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "I have tried different feature selection methods and then selected ExtraTreeClassfier feature importance method for which my models gave the highest accuracy. I am showing all the methods that I have tried for feature engineering and have commented them. They do work in improving the results but did not result in the highest accuracy\n",
        "\n",
        "Feature Engineering makes use of domain information of features to train models to solve problems. This could mean creating new features or selecting from existing features. Since all the features are numerical features which have continuous values, and the number of features is high as compared to the given number of samples, to train models it made sense to reduce features than adding new features. Without a context of the features and more information on the numerical values, its not easy to create features. The information we can infer looking at the dataset is that this is a classification problem since our target variable is always 0 or 1.\n",
        "\n",
        "Reference: https://scikit-learn.org/stable/modules/feature_selection.html\n",
        "The following methods have been used to reduce the number of features simply beacuse not all features contribute equally in helping classify a sample and features which do not contribute enough can be eliminated.\n",
        "\n",
        "1. ExtraTreesClassifier\n",
        "2. Dropping highly correlated features \n",
        "3. Lasso Regularization\n",
        "4. Principal Component Analysis\n",
        "5. SelectKBest\n",
        "6. Recursive Feature Elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHpKTdOdCC9u",
        "colab_type": "text"
      },
      "source": [
        "**Using feature importance Extra Trees Classifier**\n",
        "\n",
        "Ref: https://www.geeksforgeeks.org/ml-extra-tree-classifier-for-feature-selection/\n",
        "\n",
        "Extra Trees Classifier is based on decision trees but has low variance. Here, decision trees are created for each training sample. In these decision trees, the data is split at each node by considering random k features. They aim to find the best features to split the data by calculaing Gini Index.\n",
        "\n",
        " Based on the values of Gini Index values used for splitting, a feature importance value is calculated which is normalized reduction in Gini index. This is ordered in non-increasing order and top K features are selected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxtVLxXuKf55",
        "colab_type": "code",
        "outputId": "7d252404-9b1b-4198-d121-2b2052aa5b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Reference: https://scikit-learn.org/stable/modules/feature_selection.html\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Build a forest and compute the feature importances\n",
        "tree = ExtraTreesClassifier(n_estimators=100,\n",
        "                              random_state=10)\n",
        "\n",
        "tree.fit(x_train, y_train)\n",
        "model = SelectFromModel(tree, prefit=True)\n",
        "x_train = pd.DataFrame(model.transform(x_train))\n",
        "x_test = pd.DataFrame(model.transform(x_test))\n",
        "\n",
        "print(\"Train shape:\", x_train.shape)\n",
        "print(\"Test shape\",x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (2000, 136)\n",
            "Test shape (600, 136)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4U72Y5wtpaT",
        "colab_type": "text"
      },
      "source": [
        "**Dropping Highly Correlated Features**\n",
        "\n",
        "Features which have a correlation above a threshold of 0.95 in the train set are dropped from the train set and test set. One feature of every correlated set of features is kept in the dataset. Features which are highly correlated will not contribute anything new to the target variable of the samples and hence can be dropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oap0K91eKJI",
        "colab_type": "code",
        "outputId": "72212b09-dcad-408d-9475-7389020add35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "'''\n",
        "# https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
        "# Create correlation matrix\n",
        "corr_matrix = x_train.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find index of feature columns with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "x_train = x_train.drop(x_train[to_drop], axis=1)\n",
        "x_test = x_test.drop(x_test[to_drop], axis=1)\n",
        "print(\"train shape\", x_train.shape)\n",
        "print(\"test shape\", x_test.shape)\n",
        "print(\"Coulmns dropped:\", to_drop)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape (2000, 278)\n",
            "test shape (600, 278)\n",
            "Coulmns dropped: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR_qSKTLu6eF",
        "colab_type": "text"
      },
      "source": [
        "**Lasso Regularization**\n",
        "\n",
        "Reference: https://www.machinelearningplus.com/machine-learning/feature-selection/\n",
        "\n",
        "Lasso regression works in a way that it imposes a cost to having high coefficient values. It attempts to shrink these large coefficients in Logistic Regression. The coefficients of unwanted/ dummy features reduce to zero altogther, removing these features. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVmTriE1v8y0",
        "colab_type": "code",
        "outputId": "38a69349-86e6-4c75-8f99-484f662ed946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "'''\n",
        "# Reference: https://scikit-learn.org/stable/modules/feature_selection.html\n",
        "# Regulariztion\n",
        "\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "lsvc = LogisticRegression(C=0.01, penalty=\"l1\", dual=False).fit(x_train, y_train)\n",
        "model = SelectFromModel(lsvc, prefit=True)\n",
        "x_train = model.transform(x_train)\n",
        "x_test = model.transform(x_test)\n",
        "print(\"Train shape\",x_train.shape)\n",
        "print(\"Test shape\", x_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(2000, 345)\n",
            "(600, 345)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPI2idgblSNF",
        "colab_type": "code",
        "outputId": "27010436-3076-4546-d06f-16b3d7b0f8ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "'''\n",
        "#### https://towardsdatascience.com/dimension-reduction-techniques-with-python-f36ca7009e5c\n",
        "# https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(.95)\n",
        "\n",
        "pca.fit(x_train)\n",
        "x_train =pd.DataFrame(pca.transform(x_train))\n",
        "x_test = pd.DataFrame(pca.transform(x_test))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 278)\n",
            "(600, 278)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdA-MNIhEu5j",
        "colab_type": "text"
      },
      "source": [
        "**SelectKBest**\n",
        "\n",
        "\n",
        "This method takes a score function and number of features to keep as a parameter. Here I have used chi2 as the score function. This function is indicative that if the feature is not dependent on the target variable then it does not contribute in classification of samples and hence will have less score. The model at the end retains K features with the highest scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EValrYWOY8WX",
        "colab_type": "code",
        "outputId": "680ca7c1-b20d-475a-eae3-e76781b0dc0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "'''\n",
        "# Ref:  https://scikit-learn.org/stable/modules/feature_selection.html\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# 1. Univariate Selection\n",
        "# Statistical tests can be \n",
        "# used to select those features that have the strongest relationship with the output variable.\n",
        "\n",
        "model = SelectKBest(chi2, k = 100)\n",
        "model.fit(x_train, y_train)\n",
        "x_train =pd.DataFrame(model.transform(x_train))\n",
        "x_test = pd.DataFrame(model.transform(x_test))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 100)\n",
            "(600, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozsL5VdX1mkU",
        "colab_type": "text"
      },
      "source": [
        "**Recursive Feature Elimination**\n",
        "\n",
        "Ref: https://www.scikit-yb.org/en/latest/api/model_selection/rfecv.html#:~:targetText=Recursive%20feature%20elimination%20(RFE)%20is,number%20of%20features%20is%20reached.&targetText=RFE%20requires%20a%20specified%20number,how%20many%20features%20are%20valid.\n",
        "\n",
        "RFE has been fit to a Support Vector Classifier model. The object of this model is passed as a parameter to the RFE function. Number of features to keep can also be passed as a parameter otherwise default None considers half of the total number of features to keep. RFE ranks the features by the coefficients of the model. They are recursively eliminated based on the rank till the number of required features is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2zEVpe8QezG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "# Recursive Feature elimination\n",
        "# Takes forever to run \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "svc = SVC(kernel = 'linear', C=1)\n",
        "rfe = RFE(estimator=svc, step=1)\n",
        "rfe.fit(x_train, y_train)\n",
        "x_train =pd.DataFrame(rfe.transform(x_train))\n",
        "x_test = pd.DataFrame(rfe.transform(x_test))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQDjmxE1f85p",
        "colab_type": "text"
      },
      "source": [
        "Task 4: Model building & evaluation. Train at least 4 models:\n",
        "\n",
        "\n",
        "1.   Neural network (any type of NN is fine)\n",
        "2.   Decision tree (can be a plain decision tree, random forest, gradient boosted trees, etc.)\n",
        "3.   Support vector machine\n",
        "4.   Your choice of Naive Bayes or K-nearest neighbors\n",
        "\n",
        "For model 4, briefly (no more than 2 paragraphs) describe how the model works. \n",
        "\n",
        "Part of your grade will depend on how your best model performs against the best model of all the classmates, as determined by the accuracy achieved using the test set. You are encouraged to share your best model's accuracy on Piazza, but not which feature engineering or model tuning steps you took to achieve it. (60 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgKEx0iFjQbh",
        "colab_type": "code",
        "outputId": "c1db7098-c3a1-4209-b2d9-24f18980267e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Neural network (any type of NN is fine)\n",
        "# Decision tree (can be a plain decision tree, random forest, gradient boosted trees, etc.)\n",
        "# Support vector machine\n",
        "# Your choice of Naive Bayes or K-nearest neighbors\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 500)\n",
            "(600, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu8ZVlCsKTgF",
        "colab_type": "code",
        "outputId": "fcbde597-ecb3-4838-88cf-73f706be6ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Model 1\n",
        "# Neural Network\n",
        "# Ref: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "nn = MLPClassifier(solver = 'adam', alpha=0.1, activation= 'relu', hidden_layer_sizes=(512, 512, 512, 512), random_state=60)\n",
        "nn.fit(x_train, y_train)\n",
        "y_pred = nn.predict(x_test)\n",
        "nn_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"nn accuracy:\", nn_accuracy)\n",
        "print(\"Confusion Matrix\", metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nn accuracy: 0.73\n",
            "Confusion Matrix [[262  38]\n",
            " [124 176]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ht9fFufTXmW",
        "colab_type": "code",
        "outputId": "495d4016-6032-4c74-c97d-e0337e80973a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Model 2 - \n",
        "# Decision Tree (Gradient Boosted Trees)\n",
        "# Ref: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
        "tree = GradientBoostingClassifier(n_estimators= 250, min_samples_split= 2, validation_fraction= 0.4, random_state= 6, max_depth= 8, loss= 'deviance', learning_rate= 0.1, min_impurity_decrease = 0.01, max_features=75)\n",
        "tree.fit(x_train, y_train)\n",
        "y_pred = tree.predict(x_test)\n",
        "tree_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy:\", tree_accuracy)\n",
        "print(\"Confusion Matrix\", metrics.confusion_matrix(y_test, y_pred))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8766666666666667\n",
            "Confusion Matrix [[261  39]\n",
            " [ 35 265]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mixz-p4EDOcj",
        "colab_type": "code",
        "outputId": "ab2d1e23-60a5-49c1-8f35-464b83d26d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Model 3\n",
        "# SVM\n",
        "# Ref: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "svclassifier = SVC(kernel = 'rbf', C= 1, gamma = 1, shrinking=True) # gamma = 0.1, 1, 10, 100 \n",
        "svclassifier.fit(x_train, y_train)\n",
        "y_pred = svclassifier.predict(x_test)\n",
        "svm_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"svm accuracy:\", svm_accuracy)\n",
        "print(\"Confusion Matrix\", metrics.confusion_matrix(y_test, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "svm accuracy: 0.5\n",
            "[[  0 300]\n",
            " [  0 300]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PBl1O-dJoVo",
        "colab_type": "code",
        "outputId": "a182f02c-b4ff-4d90-f838-2463ad9a2bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Model 4\n",
        "# K nearest neighbors\n",
        "# Ref: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\n",
        "# \n",
        "knn = KNeighborsClassifier(n_neighbors = 43, weights = 'distance', algorithm='auto')\n",
        "# try what is this distance thing\n",
        "knn.fit(x_train, y_train)\n",
        "y_pred = knn.predict(x_test)\n",
        "knn_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Knn accuracy:\", knn_accuracy)\n",
        "print(\"Confusion Matrix\", metrics.confusion_matrix(y_test, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Knn accuracy: 0.8133333333333334\n",
            "Confusion Matrix [[252  48]\n",
            " [ 64 236]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTum1FbvRriQ",
        "colab_type": "text"
      },
      "source": [
        "Best model is Gradient boosted trees with accuracy of 87.66% and using ExtraTreesClassifier to select important features\n",
        "\n",
        "Model 4:\n",
        "\n",
        "K nearest neighbors:\n",
        "This model is called a lazy learner algorithm because at train time it doesn't learn anything. At the time of prediction, for a test sample, it will calculate the distance between this test sample and every sample of the train set. The distances can be euclidean, manhattan etc. It will then sort these distances in increasing order and select the top k distances. These K distances are the neighbors of this test sample and indicate that how similar they are to the test sample. The test sample is classified to the class to which maximum neighbors belong to. \n"
      ]
    }
  ]
}
